{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yi6grOMXEcgO",
    "outputId": "cef48c7b-623e-48d7-9587-c1868d60b906"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEBUG: Starting package installation...\n",
      "DEBUG: Installation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Installations ---\n",
    "# Installing all the libraries required\n",
    "print(\"DEBUG: Starting package installation...\")\n",
    "#!pip install -q google-generativeai scikit-learn pandas gradio\n",
    "!pip install -q groq scikit-learn pandas gradio\n",
    "print(\"DEBUG: Installation complete.\")\n",
    "# ----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Core Imports and Global Model Setup ---\n",
    "print(\"\\nDEBUG: Starting core imports...\")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gradio as gr\n",
    "#import google.generativeai as genai\n",
    "from groq import Groq\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "print(\"DEBUG: Imports complete.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q07F0r2eEinf",
    "outputId": "ae00dcdc-e70c-4e0f-e5ed-a2cf1ef95b1a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "DEBUG: Starting core imports...\n",
      "DEBUG: Imports complete.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ----------------------------------------------------\n",
    "# --- Model Training Section ---\n",
    "print(\"\\nDEBUG: Attempting to load and train model...\")\n",
    "model = None # Initialize model globally\n",
    "try:\n",
    "    # 1. Data Loading and Preprocessing\n",
    "    df = pd.read_csv('data.csv')\n",
    "    print(f\"DEBUG: 'data.csv' loaded successfully. Shape: {df.shape}\")\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "    df['ActualTemp'] = (df['Temp_Max'] + df['Temp_Min']) / 2\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "    df['day_of_year'] = df['Date'].dt.dayofyear\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['year'] = df['Date'].dt.year\n",
    "    print(\"DEBUG: Feature engineering complete (ActualTemp, Date components).\")\n",
    "\n",
    "    # 2. Feature and Target Setup\n",
    "    features = ['day_of_year', 'month', 'year']\n",
    "    target = 'ActualTemp'\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    # 3. Training and Evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"DEBUG: Data split into {len(X_train)} training and {len(X_test)} testing samples.\")\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    print(f\"DEBUG: Model training complete.\")\n",
    "    print(f\"\u2705 Model trained successfully on your CSV!\")\n",
    "    print(f\"Model Mean Absolute Error: {mae:.2f}\u00b0C\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\u274c Error: 'data.csv' not found. Please make sure you have uploaded the file.\")\n",
    "    model = None\n",
    "except Exception as e:\n",
    "    print(f\"\u274c An unexpected error occurred during training: {e}\")\n",
    "    model = None"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G63uMeGYEmpq",
    "outputId": "41f6a0b3-1988-48f2-e176-49d82ce3f017"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "DEBUG: Attempting to load and train model...\n",
      "DEBUG: 'data.csv' loaded successfully. Shape: (25500, 5)\n",
      "DEBUG: Feature engineering complete (ActualTemp, Date components).\n",
      "DEBUG: Data split into 20400 training and 5100 testing samples.\n",
      "DEBUG: Model training complete.\n",
      "\u2705 Model trained successfully on your CSV!\n",
      "Model Mean Absolute Error: 1.08\u00b0C\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ----------------------------------------------------\n",
    "# --- Gemini API Configuration ---\n",
    "#API_KEY = \" \" # Replace with your actual key\n",
    "#print(\"\\nDEBUG: Configuring Gemini API client.\")\n",
    "#try:\n",
    " #   genai.configure(api_key=API_KEY)\n",
    " #   llm = genai.GenerativeModel('gemini-1.5-flash')\n",
    " #   print(\"DEBUG: Gemini client initialized successfully.\")\n",
    "#except Exception as e:\n",
    "#    print(f\"\u274c Error initializing Gemini: {e}\")"
   ],
   "metadata": {
    "id": "el87YdkYEpd7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"your_groq_api_key_here\")\n",
    "print(\"\\nDEBUG: Configuring Groq API client.\")\n",
    "try:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "    print(\"DEBUG: Groq client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error initializing Groq: {e}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1uWxClPcr3M",
    "outputId": "f54771d0-9429-4e84-fba1-e51ee9793a1a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "DEBUG: Configuring Groq API client.\n",
      "DEBUG: Groq client initialized successfully.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Core Prediction Function with Streaming ---\n",
    "\n",
    "def predict_weather_for_date(target_date_str):\n",
    "    \"\"\"\n",
    "    Predicts temperature for a date string and streams LLM interpretation.\n",
    "    \"\"\"\n",
    "    print(\"\\nDEBUG: Function predict_weather_for_date started...\")\n",
    "\n",
    "    # 1. Input Parsing and Error Handling\n",
    "    try:\n",
    "        target_date = datetime.strptime(target_date_str, '%Y-%m-%d')\n",
    "        print(f\"DEBUG: Input string '{target_date_str}' successfully parsed to datetime object: {target_date.strftime('%Y-%m-%d')}\")\n",
    "    except ValueError:\n",
    "        error_msg = f\"Error: Date format is invalid. Please use YYYY-MM-DD (e.g., 2025-06-15).\"\n",
    "        print(f\"\u274c DEBUG: {error_msg}\")\n",
    "        yield error_msg, \"Invalid date format provided.\"\n",
    "        return\n",
    "\n",
    "    if model is None:\n",
    "        error_msg = \"Model not trained. Please upload 'data.csv' and run the code.\"\n",
    "        print(f\"\u274c DEBUG: {error_msg}\")\n",
    "        yield error_msg, \"Cannot provide advice. The prediction model failed to load.\"\n",
    "        return\n",
    "\n",
    "    # 2. Feature Extraction\n",
    "    day_of_year = target_date.timetuple().tm_yday\n",
    "    month = target_date.month\n",
    "    year = target_date.year\n",
    "    input_data = np.array([[day_of_year, month, year]])\n",
    "    print(f\"DEBUG: Extracted features (DoY, Month, Year): {input_data}\")\n",
    "\n",
    "    # 3. Model Prediction\n",
    "    predicted_temp = model.predict(input_data)[0]\n",
    "    print(f\"DEBUG: Prediction successful: {predicted_temp:.2f}\u00b0C\")\n",
    "    prediction_text = f\"Predicted Average Temperature for {target_date.strftime('%Y-%m-%d')}: {predicted_temp:.2f}\u00b0C\"\n",
    "\n",
    "    # Yield the numerical prediction immediately\n",
    "    yield prediction_text, \"Thinking...\"\n",
    "\n",
    "    # 4. Groq Interpretation with Streaming\n",
    "    print(\"DEBUG: Generating Groq interpretation with streaming...\")\n",
    "\n",
    "    # \ud83d\udca1 ENHANCED PROMPT TEMPLATE\n",
    "    # We now provide more context and ask for a structured response\n",
    "    prompt = f\"\"\"\n",
    "    You are a professional weather and travel advisor for Hyderabad, India.\n",
    "    Your task is to provide a concise and helpful weather summary based on the following information.\n",
    "\n",
    "    Predicted Average Temperature: {predicted_temp:.2f}\u00b0C\n",
    "    Date: {target_date.strftime('%Y-%m-%d')}\n",
    "    Location: Hyderabad, India\n",
    "\n",
    "    Please provide your response in a clear and organized format, including the following sections:\n",
    "\n",
    "    **What to Wear:** Provide one to two specific clothing recommendations.\n",
    "    **Outdoor Activities:** Suggest one to two outdoor activities suitable for the weather.\n",
    "    **General Tip:** Provide a final, brief piece of advice (e.g., related to sun safety or hydration).\n",
    "\n",
    "    Keep the tone friendly and professional.\n",
    "    \"\"\"\n",
    "\n",
    "    full_interpretation = \"\"\n",
    "    try:\n",
    "        stream = groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                full_interpretation += chunk.choices[0].delta.content\n",
    "                yield prediction_text, full_interpretation\n",
    "    except Exception as e:\n",
    "        yield prediction_text, f\"Could not get friendly advice from Groq (API Error). Error: {e}\"\n",
    "        print(f\"\u274c DEBUG: Groq API call failed: {e}\")\n",
    "\n",
    "    print(\"DEBUG: Function finished.\")\n",
    "\n"
   ],
   "metadata": {
    "id": "ZJX_pAwlEsnT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # ----------------------------------------------------\n",
    "# # --- Core Prediction Function with Streaming ---\n",
    "\n",
    "# def predict_weather_for_date(target_date_str):\n",
    "#     \"\"\"\n",
    "#     Predicts temperature for a date string and streams LLM interpretation.\n",
    "#     \"\"\"\n",
    "#     print(\"\\nDEBUG: Function predict_weather_for_date started...\")\n",
    "\n",
    "#     # 1. Input Parsing and Error Handling\n",
    "#     try:\n",
    "#         target_date = datetime.strptime(target_date_str, '%Y-%m-%d')\n",
    "#         print(f\"DEBUG: Input string '{target_date_str}' successfully parsed to datetime object: {target_date.strftime('%Y-%m-%d')}\")\n",
    "#     except ValueError:\n",
    "#         error_msg = f\"Error: Date format is invalid. Please use YYYY-MM-DD (e.g., 2025-06-15).\"\n",
    "#         print(f\"\u274c DEBUG: {error_msg}\")\n",
    "#         # Return two-element tuple for Gradio outputs\n",
    "#         yield error_msg, \"Invalid date format provided.\"\n",
    "#         return\n",
    "\n",
    "#     if model is None:\n",
    "#         error_msg = \"Model not trained. Please upload 'data.csv' and run the code.\"\n",
    "#         print(f\"\u274c DEBUG: {error_msg}\")\n",
    "#         yield error_msg, \"Cannot provide advice. The prediction model failed to load.\"\n",
    "#         return\n",
    "\n",
    "#     # 2. Feature Extraction\n",
    "#     day_of_year = target_date.timetuple().tm_yday\n",
    "#     month = target_date.month\n",
    "#     year = target_date.year\n",
    "#     input_data = np.array([[day_of_year, month, year]])\n",
    "#     print(f\"DEBUG: Extracted features (DoY, Month, Year): {input_data}\")\n",
    "\n",
    "#     # 3. Model Prediction\n",
    "#     predicted_temp = model.predict(input_data)[0]\n",
    "#     print(f\"DEBUG: Prediction successful: {predicted_temp:.2f}\u00b0C\")\n",
    "#     prediction_text = f\"Predicted Average Temperature for {target_date.strftime('%Y-%m-%d')}: {predicted_temp:.2f}\u00b0C\"\n",
    "\n",
    "#     # Yield the numerical prediction immediately\n",
    "#     yield prediction_text, \"Thinking...\"\n",
    "\n",
    "#     # 4. Gemini Interpretation with Streaming\n",
    "#     print(\"DEBUG: Generating Gemini interpretation with streaming...\")\n",
    "#     prompt = f\"\"\"\n",
    "#     The predicted average temperature is {predicted_temp:.2f}\u00b0C for the date {target_date.strftime('%Y-%m-%d')}.\n",
    "#     As a friendly weather assistant, provide a concise, single-paragraph advice based on this prediction.\n",
    "#     Focus on what to wear or what activities are appropriate.\n",
    "#     \"\"\"\n",
    "\n",
    "#     full_interpretation = \"\"\n",
    "#     try:\n",
    "#         # Use stream=True to get a response as a generator\n",
    "#         for chunk in llm.generate_content(prompt, stream=True):\n",
    "#             if chunk.text:\n",
    "#                 full_interpretation += chunk.text\n",
    "#                 yield prediction_text, full_interpretation\n",
    "#     except Exception as e:\n",
    "#         yield prediction_text, f\"Could not get friendly advice from Gemini (API Error). Error: {e}\"\n",
    "#         print(f\"\u274c DEBUG: Gemini API call failed: {e}\")\n",
    "\n",
    "#     print(\"DEBUG: Function finished.\")\n"
   ],
   "metadata": {
    "id": "RsOaYBDedjl3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# ----------------------------------------------------\n",
    "# --- Gradio Interface Launch ---\n",
    "print(\"\\nDEBUG: Preparing Gradio interface...\")\n",
    "iface = gr.Interface(\n",
    "    fn=predict_weather_for_date,\n",
    "    inputs=[gr.Textbox(label=\"Select a Date to Predict\", placeholder=\"Enter date as YYYY-MM-DD (e.g., 2025-06-15)\")],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Numerical Prediction\"),\n",
    "        gr.Textbox(label=\"Friendly Advice from Groq\")\n",
    "    ],\n",
    "    title=\"\ud83d\udcc5 Date-Based Weather Forecaster (Random Forest with Groq)\",\n",
    "    description=\"Pick a date to forecast the temperature. Requires 'data.csv' for training.\"\n",
    ")\n",
    "\n",
    "print(\"DEBUG: Launching Gradio interface...\")\n",
    "iface.queue().launch(share=True)\n",
    "print(\"DEBUG: Gradio launch call initiated.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "8Dt6-aOjE2oL",
    "outputId": "0847e213-537e-436e-ed6e-ddffe9f4d2a8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "DEBUG: Preparing Gradio interface...\n",
      "DEBUG: Launching Gradio interface...\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://ef5518abdcd9c2622e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://ef5518abdcd9c2622e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEBUG: Gradio launch call initiated.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "h3LlX60dJM6d"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}